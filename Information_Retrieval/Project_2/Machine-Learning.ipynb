{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#pandas settings\n",
    "pd.set_option('max_colwidth',250)\n",
    "pd.set_option('max_columns',250)\n",
    "pd.set_option('max_rows',500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.floor(n * multiplier) / multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/duth-dbirlab2-1/train.csv')\n",
    "test = pd.read_csv('../input/duth-dbirlab2-1/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Παρακάτω φαίνονται καποιες απο τις δοκιμές που καναμε σε κελιά με σχόλια και έχουμε αφήσει χωρίς σχόλια τα κελιά τα οποία σειριακά μας δίνουν το καλύτερο σκόρ που πετύχαμε στο Public Leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering - Current Intensity/Current Direction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['meridionalCurrent','zonalCurrent']\n",
    "names = []\n",
    "direction = []\n",
    "dir_name = []\n",
    "for var in variables:\n",
    "    names.append(train.filter(regex='^'+ var ,axis=1).filter(regex='mean$',axis=1).columns.tolist())\n",
    "for i in range(0,len(names[0])):\n",
    "    var_name = 'currentIntensity' + names[0][i].split('Current')[1]\n",
    "    dir_name.append('currentDirection' + names[0][i].split('Current')[1])\n",
    "    \n",
    "    train[var_name] = (train[names[0][i]]**2 + train[names[1][i]]**2)**(1/2)\n",
    "    test[var_name] = (test[names[0][i]]**2 + test[names[1][i]]**2)**(1/2)\n",
    "    \n",
    "    for df in [train,test]:\n",
    "        temp = []\n",
    "        for j in df.index:\n",
    "            a = df[names[0][i]][j]\n",
    "            b = df[names[1][i]][j] \n",
    "            if a > 0 :\n",
    "                if b > 0 :\n",
    "                    s = 'NE'\n",
    "                elif b < 0 :\n",
    "                    s = 'NW'\n",
    "                else:\n",
    "                    s = 'N'\n",
    "            elif a < 0 :\n",
    "                if b > 0 :\n",
    "                    s = 'SE'\n",
    "                elif b < 0 :\n",
    "                    s = 'SW'\n",
    "                else:\n",
    "                    s = 'S'\n",
    "            else:\n",
    "                if b > 0 :\n",
    "                    s = 'E'\n",
    "                elif b < 0 :\n",
    "                    s = 'W'\n",
    "                else:\n",
    "                    s = 'None'\n",
    "            temp.append(s)\n",
    "        direction.append(temp)\n",
    "\n",
    "        df.drop([names[0][i]], axis=1, inplace=True)\n",
    "        df.drop([names[1][i]], axis=1, inplace=True)\n",
    "\n",
    "        \n",
    "for i in range(0,len(direction)):\n",
    "    if (i % 2) == 0:\n",
    "        train[dir_name[0]] = direction[i]\n",
    "    else:\n",
    "        test[dir_name[0]] = direction[i]\n",
    "        dir_name.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An attempt to utilise the given variables in a better way. This script creates two new variables, Current Intensity and Current Direction , for every pair of MeridionalCurrent & ZonalCurrent mean. This method delivers a good CV mean but it doesn't improve our best run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering - Bathymetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['depth_range_0400'] = train['bathymetry'].apply(lambda x: 1 if x>-400 else 0)\n",
    "test['depth_range_0400'] = test['bathymetry'].apply(lambda x: 1 if x>-400 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a depth_range_0400 binary column that has value 1 if depth between 0-400 and 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df is just the train dataframe and then the test dataframe...\n",
    "for df in [train,test]:\n",
    "    for c in df.drop(['obs_id'],axis=1):\n",
    "        if (df[c].dtype=='object'):\n",
    "            lbl = LabelEncoder() \n",
    "            lbl.fit(list(df[c].values))\n",
    "            df[c] = lbl.transform(list(df[c].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Some useful parameters which will come in handy later on\n",
    "ntrain = train.shape[0] # or len(train) train samples length\n",
    "ntest = test.shape[0] # or len(test) test samples length\n",
    "SEED = 11 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "folds = KFold(n_splits= NFOLDS, random_state=SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 1: Replacing outliers with mean value, does not seem to work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in train.columns:\n",
    "#     temp = train[col]\n",
    "#     mean = temp.mean()\n",
    "#     q1 = train[col].quantile(.15)\n",
    "#     q3 = train[col].quantile(.85)\n",
    "#     train.loc[temp<q1,col] = mean\n",
    "#     train.loc[temp>q3,col] = mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 2: Capping the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in train.columns:\n",
    "#     temp = train[col]\n",
    "#     q1 = train[col].quantile(.03)\n",
    "#     q3 = train[col].quantile(.97)\n",
    "#     train.loc[temp<q1,col] = q1\n",
    "#     train.loc[temp>q3,col] = q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 3: Replacing value with mode of distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in train.columns:\n",
    "#     temp = train[col]\n",
    "#     q1 = train[col].quantile(.03)\n",
    "#     q3 = train[col].quantile(.97)\n",
    "#     train.loc[temp<q1,col] = temp.mode()\n",
    "#     train.loc[temp>q3,col] = temp.mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 4: A combination of caping extreme outliers and imputing the rest based on correlated variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['secchiDiskDepth','temperatureSurface','dissolvedOxygenSurface','euphoticDepth','chlorophyll']\n",
    "\n",
    "for index in range(0, len(variables)): #maybe better done with index due to list popping appending\n",
    "    mainv = variables[0] + '_mean'\n",
    "    z = np.abs(stats.zscore(train[mainv]))\n",
    "    if len(np.where(z>3)[0]) == 0:\n",
    "        temp = variables.pop(0)\n",
    "        variables.append(temp)\n",
    "        continue\n",
    "    train= train[z<3]\n",
    "    Q1 = train[mainv].quantile(0.25)\n",
    "    Q3 = train[mainv].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    out = ((train[mainv]< (Q1 - 1.5 * IQR)) | (train[mainv] > (Q3 + 1.5 * IQR)))\n",
    "    if len(np.where(out == True)[0]) == 0:\n",
    "        temp = variables.pop(0)\n",
    "        variables.append(temp)\n",
    "        continue\n",
    "    train = train.assign(outlier = list(out))\n",
    "\n",
    "    names = []\n",
    "    for var in variables:\n",
    "        names.append(train.filter(regex='^'+ var ,axis=1).filter(regex='mean$',axis=1).columns.tolist())\n",
    "\n",
    "    #names[0].remove('chlorophyll_detrend_mean') maybe remove it generally from dataset\n",
    "\n",
    "    for i in range(0,len(names[0])):\n",
    "        pt = train[train['outlier']==True ][['obs_id',names[1][i],names[2][i],names[3][i],names[4][i]]]\n",
    "        s = [names[1][i],names[2][i],names[3][i],names[4][i]]\n",
    "        new_mean = []\n",
    "        no = []\n",
    "        for id in pt['obs_id']: \n",
    "            all1 = 0.0\n",
    "            tot = 4\n",
    "            for name in s:\n",
    "                if name.startswith('euphotic'):\n",
    "                    limit = truncate(pt[name][id],2)\n",
    "                    if limit<0:\n",
    "                        sample = train[(train[name]>=limit)&(train[name]<=round(limit+0.01,2))&(train['outlier']==False) ][names[0][i]]\n",
    "                    else:\n",
    "                        sample = train[(train[name]>=limit)&(train[name]<=round(limit+0.01,2))&(train['outlier']==False) ][names[0][i]]\n",
    "                    if sample.empty:\n",
    "                        tot = tot - 1\n",
    "                        continue\n",
    "                    sm = sample.sum()\n",
    "                    c = sample.count()\n",
    "                    all1 = all1 + sm/c\n",
    "                    continue\n",
    "\n",
    "                limit = math.floor(pt[name][id])\n",
    "                sample = train[(train[name]>=limit)&(train[name]<=limit+1)&(train['outlier']==False)][names[0][i]]\n",
    "                if sample.empty:\n",
    "                        tot = tot - 1\n",
    "                        continue\n",
    "                sm = sample.sum()\n",
    "                c = sample.count()\n",
    "                all1 = all1 + sm/c\n",
    "            if(tot != 0):\n",
    "                all1 = np.around(all1/tot,decimals=6)\n",
    "                new_mean.append(all1)\n",
    "                no.append(id)\n",
    "        tot = 0\n",
    "        for num in no:\n",
    "            train.loc[[num],[names[0][i]]] = new_mean[tot]\n",
    "            tot+=1\n",
    "    \n",
    "    temp = variables.pop(0)\n",
    "    variables.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm executes the following steps:\n",
    "* Find the values that are considered as outliers and remove the extreme ones\n",
    "* For the remaining outliers check the values of the correlated variables\n",
    "* Set a range based on this values and find corresponding values for the 'main' variable\n",
    "* Replace the outlier value with the mean of the corresponding values\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['bathymetry','salinitySurface_mean','secchiDiskDepth_mean','temperatureSurface_mean','chlorophyll_mean']\n",
    "\n",
    "z = np.abs(stats.zscore(train[variables[0]]))\n",
    "train= train[z<3]\n",
    "Q1 = train[variables[0]].quantile(0.25)\n",
    "Q3 = train[variables[0]].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "out = ((train[variables[0]]< (Q1 - 1.5 * IQR)) | (train[variables[0]] > (Q3 + 1.5 * IQR)))\n",
    "train = train.assign(outlier = list(out))\n",
    "pt = train[train['outlier']==True ][['obs_id',variables[1],variables[2],variables[3],variables[4]]]\n",
    "no = []\n",
    "new_mean = []\n",
    "\n",
    "for id in pt['obs_id']:\n",
    "    all1 = 0.0\n",
    "    tot = 4\n",
    "    for i in range(1,len(variables)):     \n",
    "\n",
    "        if variables[i].startswith('chlorophyll'):\n",
    "            limit = truncate(pt[variables[i]][id],2)\n",
    "            sample = train[(train[variables[i]]>=limit)&(train[variables[i]]<=round(limit+0.01,2))&(train['outlier']==False) ][variables[0]]\n",
    "            if sample.empty:\n",
    "                tot = tot - 1\n",
    "                continue\n",
    "            sm = sample.sum()\n",
    "            c = sample.count()\n",
    "            all1 = all1 + sm/c\n",
    "            continue\n",
    "        elif variables[i].startswith('salinity'):\n",
    "            limit = math.floor(pt[variables[i]][id])\n",
    "            sample = train[(train[variables[i]]>=limit)&(train[variables[i]]<=limit+0.5)&(train['outlier']==False)][variables[0]]\n",
    "            if sample.empty:\n",
    "                tot = tot - 1\n",
    "                continue\n",
    "            sm = sample.sum()\n",
    "            c = sample.count()\n",
    "            all1 = all1 + sm/c\n",
    "            continue\n",
    "\n",
    "        limit = math.floor(pt[variables[i]][id])\n",
    "        sample = train[(train[variables[i]]>=limit)&(train[variables[i]]<=limit+1)&(train['outlier']==False)][variables[0]]\n",
    "        if sample.empty:\n",
    "            tot = tot - 1\n",
    "            continue\n",
    "        sm = sample.sum()\n",
    "        c = sample.count()\n",
    "        all1 = all1 + sm/c\n",
    "    if(tot != 0):\n",
    "        all1 = np.around(all1/tot,decimals=6)\n",
    "        new_mean.append(all1)\n",
    "        no.append(id)\n",
    "tot = 0\n",
    "for num in no:\n",
    "    train.loc[[num],[variables[0]]] = new_mean[tot]\n",
    "    tot+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting data to a numpy vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_exclude = ['obs_id','Overall Probability','outlier']\n",
    "df_train_columns = [c for c in train.columns if c not in cols_to_exclude]\n",
    "\n",
    "y_train = train['Overall Probability'].ravel() #ravel coverts a series to a numpy array\n",
    "x_train = train[df_train_columns].values # converts a dataframe to a numpy array\n",
    "x_test = test[df_train_columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the features that have low variance and transforming\n",
    "their values with log in order to spread the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# selector = VarianceThreshold(threshold=(1))\n",
    "# selector.fit(x_train)\n",
    "\n",
    "# mask = np.array(selector.get_support(),dtype='bool')\n",
    "\n",
    "# x_train_log = np.copy(x_train)\n",
    "# x_test_log = np.copy(x_test)\n",
    "# x_train_log[:,mask] = np.log10(x_train_log[:,mask]-np.amin(x_train_log[:,mask],axis = 0)+1)\n",
    "# x_test_log[:,mask] = np.log10(x_test_log[:,mask]-np.amin(x_test_log[:,mask],axis = 0)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use mainly tree based algorithms so no need to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# x_train_norm = minmax_scale(x_train)\n",
    "# x_test_norm = minmax_scale(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# scaler = scaler.fit(x_train)\n",
    "# x_train_std = scaler.transform(x_train)\n",
    "# x_test_std = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Random Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bad results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "# transformer = GaussianRandomProjection(random_state=0,n_components = 500)\n",
    "# transformer.fit(x_train)\n",
    "# x_train_gaus = transformer.transform(x_train)\n",
    "# x_test_gaus = transformer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeatureAgglomeration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import FeatureAgglomeration \n",
    "\n",
    "# selector = FeatureAgglomeration(n_clusters=5371)\n",
    "# selector.fit(x_train)\n",
    "\n",
    "# x_train_clus = selector.transform(x_train)\n",
    "# x_test_clus = selector.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# locally linear embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "# selector = LocallyLinearEmbedding(n_neighbors=50, n_components=500,random_state = 0)\n",
    "# selector.fit(x_train)\n",
    "# x_train_linemb = selector.transform(x_train)\n",
    "# x_test_linemb = selector.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components = 'mle', svd_solver='full')\n",
    "# pca.fit(x_train)\n",
    "# x_train_pca = pca.transform(x_train)\n",
    "# x_test_pca = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE, Recursive Feature Elimination\n",
    "### good but takes time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "# from timeit import default_timer\n",
    "# start = default_timer()\n",
    "\n",
    "# estimator = LinearRegression()\n",
    "# selector = RFE(estimator,n_features_to_select=2000)\n",
    "# selector = selector.fit(x_train_std,y_train)\n",
    "# mask = np.array(selector.support_,dtype=bool)\n",
    "# df_train_columns_new = np.copy(df_train_columns)\n",
    "# df_train_columns_new = np.expand_dims(df_train_columns_new, axis=0)\n",
    "# df_train_columns_new = df_train_columns_new[:,mask]\n",
    "# df_train_columns_new = np.squeeze(df_train_columns_new)\n",
    "\n",
    "\n",
    "# feature_sel_time = default_timer() - start\n",
    "\n",
    "# # can be done with transform as well\n",
    "# x_train_selected = x_train[:,mask]\n",
    "# x_test_selected = x_test[:,mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFECV \n",
    "#### RFE using cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the previous was taking time this will be endless\n",
    "\n",
    "keeping it cause it may be used for further feature selection when already having reduced the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFECV\n",
    "# estimator = RandomForestRegressor(min_weight_fraction_leaf=0.01,n_jobs=-2,random_state=0,max_depth=15, n_estimators=100)            \n",
    "# selector = RFECV(estimator)\n",
    "# selector = selector.fit(x_train,y_train)\n",
    "# mask = np.array(selector.support_,dtype=bool)\n",
    "# df_train_columns_new = np.copy(df_train_columns)\n",
    "# df_train_columns_new = np.expand_dims(df_train_columns_new, axis=0)\n",
    "# df_train_columns_new = df_train_columns_new[:,mask]\n",
    "# df_train_columns_new = np.squeeze(df_train_columns_new)\n",
    "# x_train_rfecv = x_train[:,mask]\n",
    "# x_test_rfecv = x_test[:,mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select From Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fast and good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total columns 662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "selector = ExtraTreesRegressor(min_weight_fraction_leaf=0.001,n_jobs=-2,random_state=0,max_depth=50, n_estimators=300)\n",
    "selector = selector.fit(x_train, y_train)\n",
    "# selector.feature_importances_  \n",
    "\n",
    "selector = SelectFromModel(selector, prefit=True)\n",
    "# X_new = model.transform(X)\n",
    "# X_new.shape      \n",
    "x_train_sel_model = selector.transform(x_train)\n",
    "x_test_sel_model = selector.transform(x_test)\n",
    "\n",
    "print('total columns {}'.format(np.shape(x_train_sel_model)[1]))\n",
    "\n",
    "# uncomment if you want to use lgb with columns significance \n",
    "\n",
    "# mask = np.array(selector.get_support(),dtype='bool')\n",
    "# df_train_columns_new = np.copy(df_train_columns)\n",
    "# df_train_columns_new = np.expand_dims(df_train_columns_new, axis=0)\n",
    "# df_train_columns_new = df_train_columns[:,mask]\n",
    "# df_train_columns_new = np.squeeze(df_train_columns_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance threshold \n",
    "#### removing low variance variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# selector = VarianceThreshold(threshold=(.5))\n",
    "# selector.fit(x_train)\n",
    "\n",
    "# x_train_rem_var = selector.transform(x_train)\n",
    "# x_test_rem_var = selector.transform(x_test)\n",
    "\n",
    "# print('total columns {}'.format(np.shape(x_train_rem_var)[1]))\n",
    "\n",
    "# # Reducing the labels in case it is used later on lgb\n",
    "# mask = np.array(selector.get_support(),dtype='bool')\n",
    "# df_train_columns = np.copy(df_train_columns)\n",
    "# df_train_columns_new = np.expand_dims(df_train_columns_new, axis=0)\n",
    "# df_train_columns_new = df_train_columns_new[:,mask]\n",
    "# df_train_columns_new = np.squeeze(df_train_columns_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine variance threshold with select from model technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from timeit import default_timer\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# start = default_timer()\n",
    "\n",
    "# selector = VarianceThreshold(threshold=(.5))\n",
    "# selector.fit(x_train)\n",
    "\n",
    "# x_train_rem_var = selector.transform(x_train)\n",
    "# x_test_rem_var = selector.transform(x_test)\n",
    "\n",
    "# print('total columns after variance threshold {}'.format(np.shape(x_train_rem_var)[1]))\n",
    "\n",
    "# mask = np.array(selector.get_support(),dtype='bool')\n",
    "# df_train_columns_new = np.copy(df_train_columns)\n",
    "# df_train_columns_new = np.expand_dims(df_train_columns_new, axis=0)\n",
    "# df_train_columns_new = df_train_columns_new[:,mask]\n",
    "# # df_train_columns_new = np.squeeze(df_train_columns_new)\n",
    "\n",
    "\n",
    "# start = default_timer()\n",
    "\n",
    "\n",
    "# selector = ExtraTreesRegressor(min_weight_fraction_leaf=0.001,n_jobs=-2,random_state=0,max_depth=50, n_estimators=300)\n",
    "# selector = selector.fit(x_train_rem_var, y_train)\n",
    "# # selector.feature_importances_  \n",
    "\n",
    "# selector = SelectFromModel(selector, prefit=True) \n",
    "# x_train_sel_model = selector.transform(x_train_rem_var)\n",
    "# x_test_sel_model = selector.transform(x_test_rem_var)\n",
    "\n",
    "# feature_sel_time = default_timer() - start\n",
    "# print('feature selection time: {}'.format(feature_sel_time))\n",
    "# print('total columns after select from model {}'.format(np.shape(x_train_sel_model)[1]))\n",
    "\n",
    "# mask = np.array(selector.get_support(),dtype='bool')\n",
    "# # df_train_columns_new = np.expand_dims_new(df_train_columns, axis=0)\n",
    "# df_train_columns_new = df_train_columns_new[:,mask]\n",
    "# df_train_columns_new = np.squeeze(df_train_columns_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model(X_train, X_test, Y_train, folds=5, model_type='lgb',plot_feature_importance=True,df_train_columns=df_train_columns):\n",
    "\n",
    "    oof = np.zeros(ntrain)\n",
    "    prediction = np.zeros(ntest)\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train,Y_train)):\n",
    "        print('Fold', fold_n+1, 'started at', time.ctime())\n",
    "        x_train, x_valid = X_train[train_index], X_train[valid_index]\n",
    "        y_train, y_valid = Y_train[train_index], Y_train[valid_index]\n",
    "        \n",
    "        if model_type == 'adaboost':\n",
    "            model = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "            model.fit(x_train, y_train)\n",
    "            y_pred_valid = model.predict(x_valid)\n",
    "            y_pred = model.predict(X_test) \n",
    "        \n",
    "        if model_type == 'mlp':\n",
    "            model = MLPRegressor(hidden_layer_sizes=(700,300,200,10), activation='relu', solver='adam',\n",
    "                                 alpha=1e-4, batch_size='auto', learning_rate='constant', \n",
    "                                 learning_rate_init=1e-5, power_t=0.5, max_iter=200, shuffle=True,\n",
    "                                 random_state=None, tol=0.0001, verbose=False, warm_start=False, \n",
    "                                 momentum=0.9, nesterovs_momentum=True, early_stopping=False,\n",
    "                                 validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08,\n",
    "                                 n_iter_no_change=10, max_fun=15000)\n",
    "            model.fit(x_train, y_train)\n",
    "            y_pred_valid = model.predict(x_valid)\n",
    "            y_pred = model.predict(X_test) \n",
    "        \n",
    "        if model_type == 'svm':\n",
    "            model = svm.SVR(kernel='rbf')\n",
    "            model.fit(x_train, y_train)\n",
    "            y_pred_valid = model.predict(x_valid)\n",
    "            y_pred = model.predict(X_test) \n",
    "        \n",
    "        if model_type == 'lasso':\n",
    "            model = Lasso(alpha=0.1)\n",
    "            model.fit(x_train, y_train)\n",
    "            y_pred_valid = model.predict(x_valid)\n",
    "            y_pred = model.predict(X_test) \n",
    "            \n",
    "        if model_type == 'KNN':\n",
    "            model = KNeighborsRegressor(n_neighbors = 150)\n",
    "            model.fit(x_train, y_train)\n",
    "            y_pred_valid = model.predict(x_valid)\n",
    "            y_pred = model.predict(X_test) \n",
    "        \n",
    "        if model_type == 'ridge':\n",
    "            model = Ridge(alpha=.5)\n",
    "            model.fit(x_train, y_train)\n",
    "            y_pred_valid = model.predict(x_valid)\n",
    "            y_pred = model.predict(X_test)    \n",
    "        \n",
    "        if model_type == 'linear':\n",
    "            model = LinearRegression()\n",
    "            model.fit(x_train, y_train)\n",
    "            y_pred_valid = model.predict(x_valid)\n",
    "            y_pred = model.predict(X_test) \n",
    "             \n",
    "        if model_type == 'rf': #basic\n",
    "            model = RandomForestRegressor(min_weight_fraction_leaf=0.01,n_jobs=-2,random_state=0,max_depth=15, n_estimators=100)\n",
    "            model.fit(x_train, y_train)\n",
    "            y_pred_valid = model.predict(x_valid)\n",
    "            y_pred = model.predict(X_test)               \n",
    "        \n",
    "        if model_type == 'extra_trees': #good\n",
    "            model = ExtraTreesRegressor(min_weight_fraction_leaf=0.01,n_jobs=-2,random_state=0,max_depth=20, n_estimators=100)\n",
    "            model.fit(x_train, y_train)\n",
    "            y_pred_valid = model.predict(x_valid)\n",
    "            y_pred = model.predict(X_test)  \n",
    "        \n",
    "        if model_type == 'dec_trees': ##sucks\n",
    "            model = DecisionTreeRegressor(random_state=0,max_depth=20)\n",
    "            model.fit(x_train, y_train)\n",
    "            y_pred_valid = model.predict(x_valid)\n",
    "            y_pred = model.predict(X_test)  \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            lgb_params = {   \n",
    "                         'num_leaves': 100,\n",
    "                         'extra_trees': True,\n",
    "                         'min_data_in_leaf': 10,\n",
    "                         'min_sum_hessian_in_leaf': 11,\n",
    "                         'objective': 'regression',\n",
    "                         'max_depth': -1,\n",
    "                         'learning_rate': 0.001,\n",
    "                         'boosting': \"gbdt\",\n",
    "                         'feature_fraction': 0.8,\n",
    "                         'feature_fraction_seed': 9,\n",
    "                         'max_bin ': 200,\n",
    "                         \"bagging_freq\": 5,\n",
    "                         \"bagging_fraction\": 0.8,\n",
    "                         \"bagging_seed\": 9,\n",
    "                         'metric': 'rmse',\n",
    "                         'lambda_l1': 0.1,\n",
    "                         'verbosity': -1,\n",
    "                         'min_child_weight': 5.34,\n",
    "                         'reg_alpha': 1.130,\n",
    "                         'reg_lambda': 0.360,\n",
    "                         'subsample': 0.8,\n",
    "                         }\n",
    "            \n",
    "            \n",
    "            model = lgb.LGBMRegressor(**lgb_params, n_estimators = 20000, n_jobs = -1)\n",
    "            model.fit(x_train, y_train, eval_set=[(x_train, y_train), (x_valid, y_valid)], eval_metric='rmse',verbose=10000, early_stopping_rounds=100)\n",
    "            \n",
    "            y_pred_valid = model.predict(x_valid)\n",
    "            y_pred_valid = np.clip(y_pred_valid, a_min=0, a_max=1)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            y_pred = np.clip(y_pred, a_min=0, a_max=1)\n",
    "            \n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            \n",
    "            fold_importance[\"feature\"] = train[df_train_columns].columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)            \n",
    "\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n",
    "        prediction += y_pred          \n",
    "        \n",
    "    if (model_type == 'lgb' and plot_feature_importance==True):\n",
    "\n",
    "        cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "            by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "        best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "        plt.figure(figsize=(16, 12));\n",
    "        sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "        plt.title('LGB Features (avg over folds)')\n",
    "\n",
    "    prediction /= NFOLDS        \n",
    "    print('CV mean score: {0:.5f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "\n",
    "    \n",
    "    return oof, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trials that require to take the best columns back \n",
    "def train_lgb_cv(X_train, X_test, Y_train, folds=5, model_type='lgb'):\n",
    "\n",
    "    oof = np.zeros(ntrain)\n",
    "    prediction = np.zeros(ntest)\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train,Y_train)):\n",
    "        print('Fold', fold_n+1, 'started at', time.ctime())\n",
    "        x_train, x_valid = X_train[train_index], X_train[valid_index]\n",
    "        y_train, y_valid = Y_train[train_index], Y_train[valid_index]\n",
    "        \n",
    "        lgb_params = {   \n",
    "                     'num_leaves': 100,\n",
    "                     'extra_trees': True,\n",
    "                     'min_data_in_leaf': 10,\n",
    "                     'min_sum_hessian_in_leaf': 11,\n",
    "                     'objective': 'regression',\n",
    "                     'max_depth': -1,\n",
    "                     'learning_rate': 0.001,\n",
    "                     'boosting': \"gbdt\",\n",
    "                     'feature_fraction': 0.8,\n",
    "                     'feature_fraction_seed': 9,\n",
    "                     'max_bin ': 200,\n",
    "                     \"bagging_freq\": 5,\n",
    "                     \"bagging_fraction\": 0.8,\n",
    "                     \"bagging_seed\": 9,\n",
    "                     'metric': 'rmse',\n",
    "                     'lambda_l1': 0.1,\n",
    "                     'verbosity': -1,\n",
    "                     'min_child_weight': 5.34,\n",
    "                     'reg_alpha': 1.130,\n",
    "                     'reg_lambda': 0.360,\n",
    "                     'subsample': 0.8,\n",
    "                     }\n",
    "\n",
    "\n",
    "        model = lgb.LGBMRegressor(**lgb_params, n_estimators = 20000, n_jobs = -1)\n",
    "        model.fit(x_train, y_train, eval_set=[(x_train, y_train), (x_valid, y_valid)], eval_metric='rmse',verbose=10000, early_stopping_rounds=100)\n",
    "\n",
    "        y_pred_valid = model.predict(x_valid)\n",
    "        y_pred_valid = np.clip(y_pred_valid, a_min=0, a_max=1)\n",
    "        y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "        y_pred = np.clip(y_pred, a_min=0, a_max=1)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n",
    "        prediction += y_pred          \n",
    "        \n",
    "    prediction /= NFOLDS        \n",
    "    print('CV mean score: {0:.5f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "\n",
    "    \n",
    "    return oof, prediction,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "general run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof, prediction ,best_features= train_model(X_train=x_train, X_test=x_test, Y_train=y_train, \n",
    "#                               folds=folds, model_type='lgb', plot_feature_importance=True, df_train_columns=df_train_columns)\n",
    "\n",
    "# selected_features = best_features.feature.values\n",
    "# x_train_new = train[selected_features].values \n",
    "# x_test_new = test[selected_features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof, prediction = train_model(X_train=x_train_selected, X_test=x_test_selected, Y_train=y_train, \n",
    "#                               folds=folds, model_type='lgb', plot_feature_importance=True,df_train_columns=df_train_columns_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lgb alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Mon May 18 20:20:33 2020\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6510]\ttraining's rmse: 0.0322634\tvalid_1's rmse: 0.164398\n",
      "Fold 2 started at Mon May 18 20:21:31 2020\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3789]\ttraining's rmse: 0.057435\tvalid_1's rmse: 0.159054\n",
      "Fold 3 started at Mon May 18 20:22:05 2020\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10000]\ttraining's rmse: 0.0214917\tvalid_1's rmse: 0.162511\n",
      "[20000]\ttraining's rmse: 0.00905078\tvalid_1's rmse: 0.159556\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20000]\ttraining's rmse: 0.00905078\tvalid_1's rmse: 0.159556\n",
      "Fold 4 started at Mon May 18 20:24:23 2020\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10000]\ttraining's rmse: 0.0174303\tvalid_1's rmse: 0.190683\n",
      "Early stopping, best iteration is:\n",
      "[10270]\ttraining's rmse: 0.016822\tvalid_1's rmse: 0.190624\n",
      "Fold 5 started at Mon May 18 20:25:49 2020\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10000]\ttraining's rmse: 0.0196119\tvalid_1's rmse: 0.126779\n",
      "Early stopping, best iteration is:\n",
      "[13217]\ttraining's rmse: 0.0128732\tvalid_1's rmse: 0.126204\n",
      "CV mean score: 0.15997, std: 0.0205.\n"
     ]
    }
   ],
   "source": [
    "oof, prediction = train_lgb_cv(X_train=x_train_sel_model, X_test=x_test_sel_model, Y_train=y_train\n",
    "                              ,folds=folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../input/duth-dbirlab2-1/sample_submission.csv')\n",
    "sub_df = pd.DataFrame({\"obs_id\":sample_submission[\"obs_id\"].values})\n",
    "sub_df[\"Overall Probability\"] = prediction\n",
    "sub_df[\"Overall Probability\"] = sub_df[\"Overall Probability\"].apply(lambda x: 1 if x>1 else 0 if x<0 else x)\n",
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
